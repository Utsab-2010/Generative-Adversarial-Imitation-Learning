{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04b01bd-1af1-49bf-be3c-176da86a9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import d4rl # Import required to register environments\n",
    "# import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2580782-3981-4bcb-9e48-f1f4177bfbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 19.72it/s]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"hopper-expert-v0\")\n",
    "dataset = env.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de952d19-fb43-4aaa-bac4-6021ff32224f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['observations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c58dfac-d297-497a-9245-da1c00a81233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array = np.array(dataset[\"rewards\"][:10000])\n",
    "neg_array = data_array[data_array<0]\n",
    "neg_array.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b030a412-c579-4b6f-adef-747325d2b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = torch.tensor(dataset['observations'])[:10000]\n",
    "mean = observations.mean(dim=0)\n",
    "std = observations.std(dim=0)\n",
    "observations = (observations - mean)/std\n",
    "actions = torch.tensor(dataset['actions'])[:10000]\n",
    "# data = TensorPairDataset(observations,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d5d294-d33b-4716-963b-0ad7923b273a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7744,  0.7826,  2.3278,  ...,  0.0355,  0.0036, -0.0113],\n",
       "        [-0.7768,  0.8052,  2.3472,  ...,  0.2680, -0.2564,  0.1914],\n",
       "        [-0.7831,  0.7728,  2.3604,  ...,  0.1356, -0.6247,  0.2166],\n",
       "        ...,\n",
       "        [ 1.3501, -0.4600,  0.7185,  ..., -0.8782,  0.6239,  1.1158],\n",
       "        [ 1.3850, -0.5587,  0.6150,  ..., -1.5694,  0.8177,  1.1408],\n",
       "        [ 1.4168, -0.6333,  0.5020,  ..., -1.1095,  1.2184,  1.2135]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac57a681-8024-45e8-96ef-c44c76661b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25249914e+00,  3.32176429e-03,  4.94544072e-03,  4.75809402e-03,\n",
       "       -4.01548025e-03,  2.70609150e-03, -8.25083861e-04,  1.01427586e-03,\n",
       "        4.88109332e-03, -4.73209735e-03,  3.06503894e-03])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cb3a1c0-93db-4653-9338-cf2c6598dc2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_N = {\"observations\":dataset[\"observations\"][:10000],\"actions\":dataset[\"actions\"][:10000]}\n",
    "new_dataset = [dict(zip(dataset_N.keys(), values)) for values in zip(*dataset_N.values())]\n",
    "\n",
    "# Print result\n",
    "# print(new_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e3831d-5805-446e-ae8a-5274bef8a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PM_Network(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.network_m = nn.Sequential(nn.Linear(input_size,24),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(24,8),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(8,3))                                     \n",
    "        self.network_s = nn.Sequential(nn.Linear(input_size,24),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(24,8),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(8,3))      \n",
    "    def forward(self,state):\n",
    "        self.mean = self.network_m(state)\n",
    "        self.std = F.softplus(self.network_s(state)) + 1e-6\n",
    "        # print(self.mean,self.std)\n",
    "        \n",
    "        normal_dist = torch.distributions.Normal(self.mean,self.std)\n",
    "        raw_action = normal_dist.rsample()  # Use reparameterization trick\n",
    "        \n",
    "        # Map action to [-1, 1] using tanh\n",
    "        action = torch.clamp(raw_action,min=-1,max=1)\n",
    "        # print(self.mean,self.std)\n",
    "        # Compute log probability of the sampled action\n",
    "        log_prob = normal_dist.log_prob(raw_action) \n",
    "        # print(log_prob)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)  # Sum over action dimensions\n",
    "        # print(action)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "        \n",
    "                    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d6da081-a3ae-4c66-a13b-e8c7cb495616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Network(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(D_Network,self).__init__()\n",
    "        self.network = nn.Sequential(nn.Linear(input_size,24),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(24,16),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(16,4),\n",
    "                                     nn.Tanh(),\n",
    "                                     nn.Linear(4,1),\n",
    "                                     nn.Sigmoid())\n",
    "    def forward(self,state,action):\n",
    "        # print(state,action)\n",
    "        # print(\"State shape:\", state.shape, \"Action shape:\", action.shape)\n",
    "        x = torch.cat((state, action.detach()), dim=-1)\n",
    "        # print(\"combined input\",x)\n",
    "        self.output = self.network(x.float())\n",
    "\n",
    "        return self.output\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f677c6-f976-4507-b9ad-f7dd9cf3b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy = PM_Network(11,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "993922b8-7156-4fac-a255-5ec9808b40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Discriminator = D_Network(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d1138f-bfc7-44ca-9bb2-5470429b1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce76a36b-94b8-467f-bc69-d7fbcd1089ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(dataset,epochs,traj_no,max_steps,P_network,D_network,lr_D,lr_P,lamda,opt_func=torch.optim.Adam):\n",
    "    state = torch.from_numpy(env.reset())\n",
    "    # print(state.dtype)\n",
    "    expert_trajectories =[]\n",
    "    for t in range(traj_no):\n",
    "        idx = random.randint(0,len(dataset)-max_steps)\n",
    "        expert_trajectories.append(dataset[idx:idx+max_steps])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        Ex_aD_loss = 0\n",
    "        Ex_eD_loss = 0\n",
    "        Ex_P_loss =0 \n",
    "        Ex_H_loss=0\n",
    "        # Trajectory Sampling\n",
    "        sample_trajectories =[]\n",
    "        for path in range(traj_no):\n",
    "            state = torch.from_numpy(env.reset())\n",
    "            aD_loss=0\n",
    "            eD_loss=0\n",
    "            H_loss=0\n",
    "            current_trajectory=[]\n",
    "            for step in range(1,max_steps):\n",
    "                \n",
    "                action, log_prob = P_network(state.float())\n",
    "                current_trajectory.append({\"state\":state,\"actions\":action,\"log_prob\":log_prob})\n",
    "                temp_action = action\n",
    "                next_state,reward,done,info = env.step(np.array(temp_action.detach()))\n",
    "\n",
    "                exp_state = torch.Tensor(expert_trajectories[path][step][\"observations\"])\n",
    "                exp_action = torch.Tensor(expert_trajectories[path][step][\"actions\"])\n",
    "                \n",
    "                aD_loss += torch.log(D_network(state.detach(),action.detach())+1e-8)\n",
    "                eD_loss += torch.log(1- D_network(exp_state,exp_action)+1e-8)\n",
    "\n",
    "                H_loss-=log_prob*torch.exp(log_prob).item()\n",
    "                \n",
    "                if done or step==max_steps-1:\n",
    "                    # eD_loss /= step\n",
    "                    # aD_loss /= step\n",
    "                    # P_loss /= step\n",
    "                    break\n",
    "                state = torch.from_numpy(next_state)\n",
    "                \n",
    "            sample_trajectories.append(current_trajectory)\n",
    "                    \n",
    "\n",
    "            Ex_aD_loss+=aD_loss\n",
    "            Ex_eD_loss+=eD_loss\n",
    "            Ex_H_loss += H_loss\n",
    "        \n",
    "        ### Updating Discriminator \n",
    "        print(\"XX============UPDATE DISCRIMINATOR===============XX\")\n",
    "        Ex_aD_loss/=traj_no\n",
    "        Ex_eD_loss/=traj_no\n",
    "        total_D_loss = -(Ex_aD_loss + Ex_eD_loss)\n",
    "        \n",
    "        \n",
    "        opt_D = opt_func(D_network.parameters(),lr_D)\n",
    "        # print(Ex_aD_loss,Ex_eD_loss)\n",
    "        print(\"total D loss\",total_D_loss)\n",
    "        # print(list(D_network.parameters()))\n",
    "        total_D_loss.backward()\n",
    "        opt_D.step()\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        \n",
    "        ## Updating the Policy\n",
    "        print(\"XX================UPDATE POLICY======================XX\")\n",
    "        Ex_P_loss=0\n",
    "        \n",
    "        for traj in sample_trajectories:\n",
    "            traj_P_loss=0\n",
    "            for idx,step in enumerate(traj):\n",
    "                Q=0.1\n",
    "                for jdx in range(idx+1,len(traj)):\n",
    "                    temp = D_network(traj[jdx]['state'].detach(),traj[jdx]['actions'].detach())\n",
    "                    # print(\"temp\",temp)\n",
    "                    Q+=torch.log(temp).item()\n",
    "                    \n",
    "                # print(\"Q:\",Q)\n",
    "                traj_P_loss+=step['log_prob']*Q\n",
    "            \n",
    "            Ex_P_loss+=traj_P_loss\n",
    "        \n",
    "                \n",
    "        Ex_P_loss/=traj_no\n",
    "        # Ex_H_loss/=traj_no\n",
    "        # Ex_H_loss/=traj_no\n",
    "        total_P_loss = (Ex_P_loss) - lamda*Ex_H_loss \n",
    "        print(\"PLOss:\",total_P_loss)\n",
    "        opt_P = opt_func(P_network.parameters(),lr_P)\n",
    "        total_P_loss.backward()\n",
    "        opt_P.step()\n",
    "        opt_P.zero_grad()\n",
    "        print(\"P descent happened!!\")\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f8c2fd3-2d8a-4d99-a111-feb47660658b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([66.1064], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1381.8131], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([72.4726], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1587.6176], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([60.0060], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1137.4939], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([62.9477], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1319.0419], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([60.3831], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1233.6179], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([65.4535], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1655.8586], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([55.6413], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1120.8329], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([53.0847], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1099.3429], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([47.9890], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([943.1318], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([54.3620], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1195.8815], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([46.0760], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([895.7736], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([45.1609], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1007.7424], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([40.7463], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([623.3670], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([40.3264], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([634.7518], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([33.5989], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([497.1566], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([38.1841], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([734.8265], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([34.7935], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([600.4338], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([37.5457], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([675.1282], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([32.0785], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([553.4742], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([31.5238], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([498.1868], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([29.1301], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([472.1592], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([29.4855], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([485.5066], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([28.0877], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([324.4597], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([30.5970], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([573.9849], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([22.7698], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([234.7026], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([22.2487], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([214.8462], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([23.1381], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([299.2538], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([27.4664], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([246.6377], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([24.4159], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([277.9932], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([23.0982], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([256.8531], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([20.9360], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([250.0409], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([19.7655], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([226.5731], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([19.6921], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([159.5402], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([15.0966], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([127.8440], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([17.0468], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([125.8596], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([18.0819], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([140.5945], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([14.1918], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([81.7378], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([15.9479], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([119.3178], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([14.9923], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([82.9176], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([12.6790], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([86.7732], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([15.8763], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([109.8677], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([13.5047], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([84.7725], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([15.1230], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([72.4616], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([14.1409], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([72.2381], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([13.9793], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([42.8451], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.0812], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([26.7450], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([12.6558], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([83.8800], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([12.3192], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([33.6571], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([12.0973], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([90.1201], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([11.5537], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([24.6367], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([10.7051], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([14.9876], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([11.2694], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([11.3327], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.8431], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([28.4879], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([12.8624], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([22.0938], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([10.0886], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([24.7371], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.9382], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([9.4203], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([10.3729], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([19.3807], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.7996], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([4.8551], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.1161], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-5.9652], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([11.6523], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([14.4063], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.2841], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([0.6263], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.4867], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([2.3460], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([8.1402], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-6.0057], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([10.3657], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([0.8021], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.4520], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([0.1303], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.6868], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1.1048], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.1911], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-1.4190], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.1800], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-6.9514], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.8205], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-3.2156], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.4049], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-2.1354], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([9.3979], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-0.7712], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.3490], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-7.7869], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.3701], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-2.7952], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([10.9034], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-4.7834], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.1920], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-0.9510], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.5664], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([-3.9797], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.3499], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([2.4236], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.7125], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([1.3535], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.1491], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([3.6835], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([5.4078], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([4.8185], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([5.7591], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([8.2585], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.9643], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([9.6628], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.6159], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([11.4061], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([7.5846], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([16.3777], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.5502], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([17.8110], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([8.5845], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([20.2887], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.8031], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([22.9691], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.0917], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([17.4224], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.8405], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([18.4865], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([4.6216], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([20.8254], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.8122], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([19.6859], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.7757], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([19.7798], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.3827], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([18.5314], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.3568], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([18.8386], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([4.1438], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([21.5799], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([4.3902], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([23.4863], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([8.9869], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([19.1768], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.6853], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([15.0430], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([5.5306], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([17.8725], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n",
      "XX============UPDATE DISCRIMINATOR===============XX\n",
      "total D loss tensor([6.2782], grad_fn=<NegBackward0>)\n",
      "XX================UPDATE POLICY======================XX\n",
      "PLOss: tensor([17.3494], grad_fn=<SubBackward0>)\n",
      "P descent happened!!\n"
     ]
    }
   ],
   "source": [
    "train(new_dataset,100,10,300,Policy,Discriminator,1e-2,1e-3,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18afbaaf-d056-4e1f-89c7-31e6a802fc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (3,), float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f3fdb739-8e08-4f11-b02e-af3d8061be45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0253, -0.0562, -0.0464], grad_fn=<ClampBackward1>),\n",
       " tensor([3.9466], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Policy(state.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dd299ca-e6bb-4e25-abe8-27bb89f21869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,D_network,episodes):\n",
    "    env = gym.make(\"hopper-expert-v0\")\n",
    "    # state,info = env.reset()\n",
    "    state = torch.from_numpy(env.reset())\n",
    "    total_reward=0\n",
    "    D_loss =0\n",
    "    for ep in range(episodes):\n",
    "       \n",
    "        action, log_prob = model(state.float())\n",
    "        temp_action = action\n",
    "        next_state,reward,done,info = env.step(np.array(temp_action.detach()))\n",
    "        D_loss += torch.log(D_network(state.detach(),action.detach())+1e-8).item()\n",
    "        total_reward+=reward\n",
    "        if done or ep==episodes-1:\n",
    "            print(total_reward,ep,D_loss)\n",
    "            \n",
    "            break\n",
    "        state = torch.from_numpy(next_state)    \n",
    "    env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e20af6c1-8412-425c-983b-e6cbafe52782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.1800582912413 39 -0.8850178867578506\n",
      "78.16346447767653 49 -1.1062723584473133\n",
      "73.42365866834382 44 -0.995645122602582\n",
      "72.93741231893219 49 -1.1062723584473133\n",
      "69.74051002100086 41 -0.9292687810957432\n",
      "68.51393076983254 41 -0.9292687810957432\n",
      "64.8223833661753 38 -0.8628924395889044\n",
      "69.38194206856501 42 -0.9513942282646894\n",
      "77.5262960162779 49 -1.1062723584473133\n",
      "75.22054518555791 45 -1.0177705697715282\n",
      "60.66736071436634 36 -0.8186415452510118\n",
      "65.73002497388603 39 -0.8850178867578506\n",
      "68.45210880422424 40 -0.9071433339267969\n",
      "73.53251916926321 44 -0.995645122602582\n",
      "72.1538682413839 43 -0.9735196754336357\n",
      "73.3731325388231 44 -0.995645122602582\n",
      "77.63000783666065 49 -1.1062723584473133\n",
      "63.86802059181661 38 -0.8628924395889044\n",
      "66.8414912263664 40 -0.9071433339267969\n",
      "76.83695148143843 46 -1.0398960169404745\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    test(Policy,Discriminator,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15fb9de8-7edf-475c-b88e-a87fdc80716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy_random = PM_Network(11,3)\n",
    "D_random = D_Network(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de4b08d8-ce81-4623-bf24-d92dd584ac4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.881493748814688 8 -5.584661364555359\n",
      "5.979921445232856 7 -5.030848741531372\n",
      "13.379352816140864 15 -10.337769269943237\n",
      "8.765036066981214 9 -6.272688925266266\n",
      "9.90976770242666 9 -6.220690667629242\n",
      "9.160820276921338 10 -7.0389769077301025\n",
      "6.499856430808439 8 -5.742412090301514\n",
      "13.452229521565847 14 -9.704015374183655\n",
      "8.015553724005388 10 -7.068350195884705\n",
      "5.04691883299593 11 -7.924397826194763\n",
      "8.484104750842096 10 -7.06064110994339\n",
      "6.275024420845617 10 -7.1368520855903625\n",
      "4.320179777452697 6 -4.407621383666992\n",
      "5.240018461710615 9 -6.480333924293518\n",
      "8.197756770945944 9 -6.365648984909058\n",
      "9.3885599189925 10 -6.9461177587509155\n",
      "7.757151341306446 9 -6.3882155418396\n",
      "10.730500608002185 11 -7.6920687556266785\n",
      "7.384956126754444 10 -7.136242747306824\n",
      "7.5382089593371795 8 -5.655667662620544\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    test(Policy_random,D_random,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe32258-e102-46ba-8c03-7be724fea02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IRL_AGV",
   "language": "python",
   "name": "irl_agv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
